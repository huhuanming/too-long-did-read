---
titleEn: "Sharing Our Compliance Framework for California's Transparency in Frontier AI Act"
titleZh: "分享我们针对加州前沿 AI 透明度法案的合规框架"
date: 2025-12-19
slug: compliance-framework-SB53
originalUrl: https://www.anthropic.com/news/compliance-framework-SB53
---

## English

On January 1, California's Transparency in Frontier AI Act (SB 53) will go into effect. It establishes the nation's first frontier AI safety and transparency requirements for catastrophic risks.

While we have long advocated for a federal framework, Anthropic endorsed SB 53 because frontier AI developers should be transparent about assessing and managing risks. The law balances strong safety practices, incident reporting, and whistleblower protections while preserving implementation flexibility and exempting smaller companies from regulatory burdens.

Frontier AI developers must publish frameworks describing their catastrophic risk assessment and management. Anthropic's Frontier Compliance Framework (FCF) is now publicly available. It describes how Anthropic assesses and mitigates cyber offense, chemical, biological, radiological, and nuclear threats, plus AI sabotage and loss of control risks for frontier models. The framework details a tiered capability evaluation system, mitigation approaches, model weight protection, and safety incident response procedures.

Much of the FCF reflects evolved practices from prior years. Since 2023, Anthropic's Responsible Scaling Policy (RSP) outlined approaches to managing extreme risks from advanced AI systems. Under the new law, such transparency practices become mandatory for developers building California's most powerful AI systems.

### Federal Framework Need

SB 53 implementation formalizes transparency practices that responsible labs already follow voluntarily, ensuring commitments cannot be quietly abandoned as models become more capable. A federal AI transparency framework is now needed for nationwide consistency.

Anthropic proposed a federal legislation framework emphasizing public visibility into safety practices without locking in specific technical approaches. Core tenets include:

- **Public secure development framework:** Covered developers should publish frameworks outlining their assessment and mitigation of serious risks
- **System cards at deployment:** Testing, evaluation procedures, results, and mitigation documentation should be publicly disclosed
- **Whistleblower protection:** Labs lying about framework compliance or punishing employees raising concerns should face legal violations
- **Flexible transparency standards:** Standards should be lightweight, adaptable requirements as consensus best practices emerge
- **Largest developer application limits:** Requirements should apply only to established frontier developers building most capable models

As AI systems grow more powerful, the public deserves visibility into development processes and safeguards in place.

---

## 中文翻译

1 月 1 日，加州的《前沿 AI 透明度法案》(SB 53) 将生效。它建立了全国首个针对灾难性风险的前沿 AI 安全和透明度要求。

虽然我们长期倡导联邦框架，但 Anthropic 支持 SB 53，因为前沿 AI 开发者应该对评估和管理风险保持透明。该法律在强有力的安全实践、事件报告和举报人保护之间取得了平衡，同时保持了实施灵活性并豁免了较小公司的监管负担。

前沿 AI 开发者必须发布描述其灾难性风险评估和管理的框架。Anthropic 的前沿合规框架 (FCF) 现已公开发布。它描述了 Anthropic 如何评估和缓解网络攻击、化学、生物、放射性和核威胁，以及前沿模型的 AI 破坏和失控风险。该框架详细说明了分层能力评估系统、缓解方法、模型权重保护和安全事件响应程序。

FCF 的大部分内容反映了前几年的演进实践。自 2023 年以来，Anthropic 的负责任扩展政策 (RSP) 概述了管理高级 AI 系统极端风险的方法。根据新法律，此类透明度实践对于构建加州最强大 AI 系统的开发者来说变为强制性的。

### 联邦框架需求

SB 53 的实施将负责任实验室已经自愿遵循的透明度实践正式化，确保承诺不会随着模型变得更强大而被悄然放弃。现在需要一个联邦 AI 透明度框架以实现全国一致性。

Anthropic 提出了一个联邦立法框架，强调对安全实践的公众可见性，同时不锁定特定技术方法。核心原则包括：

- **公共安全开发框架：** 受覆盖的开发者应发布概述其评估和缓解严重风险的框架
- **部署时的系统卡：** 测试、评估程序、结果和缓解文档应公开披露
- **举报人保护：** 对框架合规性撒谎或惩罚提出担忧的员工的实验室应面临法律违规
- **灵活的透明度标准：** 标准应是轻量级的、可适应的要求
- **最大开发者适用限制：** 要求应仅适用于构建最强大模型的成熟前沿开发者

随着 AI 系统变得更加强大，公众有权了解开发过程和现有的保障措施。
